{"cells":[{"cell_type":"markdown","source":["# Mounting drive"],"metadata":{"id":"nCqNcPRKHhL2"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y03PRYs5P2Gt","outputId":"ed7bebbc-18f3-4a77-9a10-50466769d745","executionInfo":{"status":"ok","timestamp":1707082096748,"user_tz":-60,"elapsed":16624,"user":{"displayName":"Tomasz Dołmat","userId":"04722378432579170082"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lhBwTILvHlOM","outputId":"e070b5cb-3b38-4116-d23d-0123243b020c","executionInfo":{"status":"ok","timestamp":1707082070323,"user_tz":-60,"elapsed":132483,"user":{"displayName":"Tomasz Dołmat","userId":"04722378432579170082"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting flashlight-text\n","  Downloading flashlight_text-0.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: flashlight-text\n","Successfully installed flashlight-text-0.0.4\n","Collecting kenlm\n","  Downloading kenlm-0.2.0.tar.gz (427 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.4/427.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: kenlm\n","  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kenlm: filename=kenlm-0.2.0-cp310-cp310-linux_x86_64.whl size=3184421 sha256=07ff8fed5dbab3998a901586e832d6abff385bc0718f2f00ffcf82cc93cd9d04\n","  Stored in directory: /root/.cache/pip/wheels/fd/80/e0/18f4148e863fb137bd87e21ee2bf423b81b3ed6989dab95135\n","Successfully built kenlm\n","Installing collected packages: kenlm\n","Successfully installed kenlm-0.2.0\n","Collecting jiwer\n","  Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n","Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n","Collecting rapidfuzz<4,>=3 (from jiwer)\n","  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n","Successfully installed jiwer-3.0.3 rapidfuzz-3.6.1\n"]}],"source":["!pip install flashlight-text\n","!pip install kenlm\n","!pip install jiwer"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"337KbfrWHIA_","executionInfo":{"status":"ok","timestamp":1707082080126,"user_tz":-60,"elapsed":9813,"user":{"displayName":"Tomasz Dołmat","userId":"04722378432579170082"}}},"outputs":[],"source":["import time\n","from typing import List\n","\n","import IPython\n","import matplotlib.pyplot as plt\n","from torchaudio.models.decoder import ctc_decoder\n","from torchaudio.utils import download_asset\n","\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as data\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchaudio\n","import numpy as np\n","import pandas as pd\n","from jiwer import wer, cer"]},{"cell_type":"code","source":["# AUDIO\n","SAMPLE_RATE = 32000\n","\n","# MEL LOG SPECTROGRAM\n","N_MELS = 128\n","N_FFT = 1024\n","WIN_LENGTH = 1024\n","HOP_LENGTH = 512\n","MAX_SPECTROGRAM_SIZE = 1650\n","\n","# SPECTROGRAM AUGMENTATION\n","SPECAUG_RATE = 0.5\n","SPECAUG_POLICY = 3\n","TIME_MASK = 60\n","FREQUENCY_MASK = 20\n","\n","# TEXT\n","NUMBER_OF_CLASSES = 29 # number of label clases (characters)\n","BLANK_CHARACTER_INDEX = 28\n","\n","# MODEL\n","DROPOUT = 0.1\n","MAIN_SIZE = 128\n","\n","# CNN\n","KERNEL_SIZE = 10\n","STRIDE = 2\n","\n","# LSTM\n","LSTM_HIDDEN_SIZE = 512\n","LSTM_NUMBER_OF_LAYERS = 1\n","LSTM_DROPOUT = 0.0\n","LSTM_BIDIRECTIONAL = False"],"metadata":{"id":"hqujb0b4AayK","executionInfo":{"status":"ok","timestamp":1707083743944,"user_tz":-60,"elapsed":228,"user":{"displayName":"Tomasz Dołmat","userId":"04722378432579170082"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["# Acoustic model"],"metadata":{"id":"pe1Kt2x3AmYW"}},{"cell_type":"code","source":["class TransposeLayer(nn.Module):\n","    def __init__(self, dim0, dim1):\n","        super(TransposeLayer, self).__init__()\n","        self.dim0 = dim0\n","        self.dim1 = dim1\n","\n","    def forward(self, x):\n","        x = x.transpose(self.dim0, self.dim1)\n","        return x\n","\n","\n","class SpeechRecognitionModel(nn.Module):\n","\n","    def __init__(self, device=\"cpu\"):\n","        super(SpeechRecognitionModel, self).__init__()\n","\n","        use_cuda = torch.cuda.is_available()\n","        self.device = device\n","\n","        self.criterion = nn.CTCLoss(blank=28, zero_infinity=True)\n","        self.learning_rate = 1e-3\n","\n","        self.validation_step_outputs = []\n","\n","        self.cnn = nn.Sequential(\n","            nn.Conv1d(N_MELS, N_MELS, kernel_size=KERNEL_SIZE, stride=STRIDE, padding=KERNEL_SIZE//STRIDE),\n","            TransposeLayer(1, 2),\n","            nn.LayerNorm(N_MELS),\n","            nn.GELU(),\n","            nn.Dropout(DROPOUT),\n","        )\n","        self.dense = nn.Sequential(\n","            nn.Linear(N_MELS, 128),\n","            nn.LayerNorm(128),\n","            nn.GELU(),\n","            nn.Dropout(DROPOUT),\n","            nn.Linear(128, 128),\n","            nn.LayerNorm(128),\n","            nn.GELU(),\n","            nn.Dropout(DROPOUT),\n","        )\n","        self.lstm = nn.LSTM(input_size=128, hidden_size=LSTM_HIDDEN_SIZE,\n","                            num_layers=LSTM_NUMBER_OF_LAYERS, dropout=LSTM_DROPOUT,\n","                            bidirectional=LSTM_BIDIRECTIONAL, batch_first=True)\n","        self.final_transformations = nn.Sequential(\n","            nn.LayerNorm(LSTM_HIDDEN_SIZE),\n","            nn.GELU(),\n","            nn.Dropout(DROPOUT),\n","        )\n","\n","        self.final_fc = nn.Linear(LSTM_HIDDEN_SIZE, NUMBER_OF_CLASSES) # final fully connected\n","\n","    def forward(self, x):\n","        current_batch_size = x.shape[0]\n","\n","        h_0 = torch.zeros(1, current_batch_size, LSTM_HIDDEN_SIZE).to(self.device)\n","        c_0 = torch.zeros(1, current_batch_size, LSTM_HIDDEN_SIZE).to(self.device)\n","\n","        x = x.squeeze(1)  # batch, feature, time - removing unnecessary dimention for num_of_channels\n","        x = self.cnn(x) # batch, time, feature\n","        x = self.dense(x) # batch, time, feature\n","        x, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n","\n","        x = self.final_transformations(x)  # (batch, time, n_class)\n","        x = self.final_fc(x)\n","        return x\n"],"metadata":{"id":"UurIcJDaAdrT","executionInfo":{"status":"ok","timestamp":1707082080127,"user_tz":-60,"elapsed":4,"user":{"displayName":"Tomasz Dołmat","userId":"04722378432579170082"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ps7MBPtOHIBC"},"source":["### Beam Search Decoder\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"86-q56QJHIBC"},"outputs":[],"source":["LM_WEIGHT = 3.23\n","WORD_SCORE = -0.26\n","\n","beam_search_decoder = ctc_decoder(\n","    lexicon=\"/content/drive/MyDrive/speech_recognition/lexicon.txt\",\n","    tokens=\"/content/drive/MyDrive/speech_recognition/tokens.txt\",\n","    lm=\"/content/drive/MyDrive/speech_recognition/lm.bin\",\n","    nbest=3,\n","    beam_size=1500,\n","    lm_weight=LM_WEIGHT,\n","    word_score=WORD_SCORE,\n",")"]},{"cell_type":"markdown","metadata":{"id":"ViAp_BvwHIBC"},"source":["### Greedy Decoder\n","\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"JUzgPiucHIBC","executionInfo":{"status":"ok","timestamp":1707083783957,"user_tz":-60,"elapsed":209,"user":{"displayName":"Tomasz Dołmat","userId":"04722378432579170082"}}},"outputs":[],"source":["tokens = [\"'\", ' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_']\n","\n","\n","class GreedyCTCDecoder(torch.nn.Module):\n","    # def __init__(self, labels, blank=0):\n","    def __init__(self, labels, blank=BLANK_CHARACTER_INDEX):\n","        super().__init__()\n","        self.labels = labels\n","        self.blank = blank\n","\n","    def forward(self, emission: torch.Tensor) -> List[str]:\n","        indices = torch.argmax(emission, dim=-1)\n","        indices = torch.unique_consecutive(indices, dim=-1)\n","        indices = [i for i in indices if i != self.blank]\n","        return \"\".join([self.labels[i] for i in indices])\n","\n","\n","greedy_decoder = GreedyCTCDecoder(tokens)"]},{"cell_type":"markdown","source":["# Acoustic model prediction"],"metadata":{"id":"CSSovvF8HYTi"}},{"cell_type":"code","source":["acoustic_mode = SpeechRecognitionModel()\n","\n","def get_prediction(waveform):\n","    waveform, sample_rate = torchaudio.load(\"/content/drive/MyDrive/speech_recognition/data/cv-corpus-small/clips-wav/common_voice_en_38334309.wav\")\n","\n","    mel_spec_fn = torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=N_MELS)\n","\n","    spectrogram = mel_spec_fn(waveform)\n","    spectrogram = spectrogram.unsqueeze(0) # to simulate batch for model\n","\n","    with torch.no_grad():\n","        mod = SpeechRecognitionModel()\n","\n","        output = mod(spectrogram)\n","        output = F.log_softmax(output, dim=2)\n","\n","        return output"],"metadata":{"id":"-jrCLgtMBxEL","executionInfo":{"status":"ok","timestamp":1707083786056,"user_tz":-60,"elapsed":230,"user":{"displayName":"Tomasz Dołmat","userId":"04722378432579170082"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["# Get decoding results"],"metadata":{"id":"Yb15Gx28H7cV"}},{"cell_type":"code","execution_count":19,"metadata":{"id":"nkNN7Scfe2O9","executionInfo":{"status":"ok","timestamp":1707083594182,"user_tz":-60,"elapsed":227,"user":{"displayName":"Tomasz Dołmat","userId":"04722378432579170082"}}},"outputs":[],"source":["def get_greedy_result(model_output):\n","    return \" \".join(greedy_decoder(model_output[0]))\n","\n","def get_beam_search_result(model_output):\n","    return \" \".join(beam_search_decoder(model_output)[0][0].words).strip()"]},{"cell_type":"markdown","metadata":{"id":"-Q6_M7B8HIBC"},"source":["## Run\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VkLw9VIEXVoA"},"outputs":[],"source":["import pandas as pd\n","import csv\n","from jiwer import wer, cer\n","\n","data = pd.read_csv(\"/content/drive/MyDrive/speech_recognition/data/cv-corpus-small/datasets-csv-colab/test.csv\", sep='\\t')\n","\n","LIMITER = None\n","\n","iterations = len(data) if not LIMITER else LIMITER\n","\n","save_data_labels = ['actual', 'greedy', 'beam_search',\n","                    'greedy_wer', 'greedy_cer',\n","                    'beam_search_wer', 'beam_search_cer']\n","save_data = []\n","\n","erorrs_data_labels = ['', 'greedy', 'beam_search']\n","erorrs_data = []\n","\n","for i in range(iterations):\n","    current_data_row = data.iloc[i]\n","    actual_transcript = current_data_row.text\n","\n","    file_path = current_data_row.file\n","    waveform, sample_rate = torchaudio.load(file_path)\n","\n","    model_output = get_prediction(waveform)\n","\n","    greedy_result = get_greedy_result(model_output)\n","    beam_search_result = get_beam_search_result(model_output)\n","\n","    greedy_wer = wer(actual_transcript, greedy_result)\n","    beam_search_wer = wer(actual_transcript, beam_search_result)\n","\n","    greedy_cer = cer(actual_transcript, greedy_result)\n","    beam_search_cer = cer(actual_transcript, beam_search_result)\n","\n","    save_data.append([actual_transcript, greedy_result, beam_search_result,\n","                      greedy_wer, greedy_cer,\n","                      beam_search_wer, beam_search_cer])\n","\n","    print(f\"Data collected: {i+1}/{iterations}\")\n","\n","    with open(\"/content/drive/MyDrive/speech_recognition/result_comparison.csv\", \"w\") as train_file:\n","        train_writer = csv.writer(train_file, delimiter='\\t')\n","        train_writer.writerow(save_data_labels)\n","        train_writer.writerows(save_data)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}