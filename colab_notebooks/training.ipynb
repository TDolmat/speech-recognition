{"cells":[{"cell_type":"markdown","metadata":{"id":"JeRgufT8I2XI"},"source":["# Mounting drive"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ulD5D2PCI4g5","outputId":"9a351df0-f812-401c-95c7-d9ea02dcc31a","executionInfo":{"status":"ok","timestamp":1707077705136,"user_tz":-60,"elapsed":17597,"user":{"displayName":"Tomasz Dołmat","userId":"04722378432579170082"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"q1fXgsDQmK09"},"source":["## installing the requirements"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gwfN8o17Bdp2","outputId":"8ceb04eb-3d0e-4f47-eb2a-49c821c1ad6a","executionInfo":{"status":"ok","timestamp":1707077725500,"user_tz":-60,"elapsed":20367,"user":{"displayName":"Tomasz Dołmat","userId":"04722378432579170082"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0) (2.1.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n","Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.1.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchaudio) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchaudio) (2.1.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchaudio) (1.3.0)\n","Collecting jiwer\n","  Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n","Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n","Collecting rapidfuzz<4,>=3 (from jiwer)\n","  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n","Successfully installed jiwer-3.0.3 rapidfuzz-3.6.1\n"]}],"source":["!pip install torch==2.1.0\n","!pip install torchaudio\n","!pip install jiwer"]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"_uSsg6qAw6nG"}},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.utils.data as data\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchaudio\n","import numpy as np\n","import pandas as pd\n","from jiwer import wer, cer"],"metadata":{"id":"HGaUTkZCw2iB","executionInfo":{"status":"ok","timestamp":1707078049287,"user_tz":-60,"elapsed":262,"user":{"displayName":"Tomasz Dołmat","userId":"04722378432579170082"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["MAX_EPOCHS = 100\n","ON_MAC = False\n","\n","\n","# PATHS\n","TRAIN_PATH = \"/content/drive/MyDrive/speech_recognition/data/cv-corpus/datasets-csv-colab/train.csv\"\n","VALIDATE_PATH = \"/content/drive/MyDrive/speech_recognition/data/cv-corpus/datasets-csv-colab/test.csv\"\n","SAVE_MODEL_PATH = \"/content/drive/MyDrive/speech_recognition/model\"\n","\n","# DATA\n","BATCH_SIZE = 64\n","VALID_EVERY = 1000 // BATCH_SIZE\n","\n","# DATALOADER\n","NUM_WORKERS = 1\n","\n","# AUDIO\n","SAMPLE_RATE = 32000\n","\n","# MEL LOG SPECTROGRAM\n","N_MELS = 128\n","N_FFT = 1024\n","WIN_LENGTH = 1024\n","HOP_LENGTH = 512\n","MAX_SPECTROGRAM_SIZE = 1650\n","\n","# SPECTROGRAM AUGMENTATION\n","SPECAUG_RATE = 0.5\n","SPECAUG_POLICY = 3\n","TIME_MASK = 60\n","FREQUENCY_MASK = 20\n","\n","# TEXT\n","NUMBER_OF_CLASSES = 29 # number of label clases (characters)\n","BLANK_CHARACTER_INDEX = 28\n","\n","# MODEL\n","DROPOUT = 0.1\n","MAIN_SIZE = 128\n","\n","# CNN\n","KERNEL_SIZE = 10\n","STRIDE = 2\n","\n","# LSTM\n","LSTM_HIDDEN_SIZE = 512\n","LSTM_NUMBER_OF_LAYERS = 1\n","LSTM_DROPOUT = 0.0\n","LSTM_BIDIRECTIONAL = False"],"metadata":{"id":"PBsuH4xZ6f7D","executionInfo":{"status":"ok","timestamp":1707080843966,"user_tz":-60,"elapsed":299,"user":{"displayName":"Tomasz Dołmat","userId":"04722378432579170082"}}},"execution_count":67,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HxRIb_WempDq"},"source":["# GPU runtime"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nlUSuAJwlzo8","outputId":"829ff0b5-0130-4f99-d89e-2678b6177091","executionInfo":{"status":"ok","timestamp":1707077732623,"user_tz":-60,"elapsed":384,"user":{"displayName":"Tomasz Dołmat","userId":"04722378432579170082"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Feb  4 20:15:32 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   54C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"tSKHvy8DmOCQ"},"source":["# Text processing"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"RVJs4Bk8FjjO","executionInfo":{"status":"ok","timestamp":1707077726014,"user_tz":-60,"elapsed":517,"user":{"displayName":"Tomasz Dołmat","userId":"04722378432579170082"}}},"outputs":[],"source":["class TextProcessing():\n","    index_to_char_map = {\n","        0: \"'\", 1: \" \", 2: \"a\", 3: \"b\", 4: \"c\", 5: \"d\", 6: \"e\", 7: \"f\", 8: \"g\", 9: \"h\", 10: \"i\",\n","        11: \"j\", 12: \"k\", 13: \"l\", 14: \"m\", 15: \"n\", 16: \"o\", 17: \"p\", 18: \"q\", 19: \"r\", 20: \"s\",\n","        21: \"t\", 22: \"u\", 23: \"v\", 24: \"w\", 25: \"x\", 26: \"y\", 27: \"z\", 28: \"_\", # blank character\n","    }\n","    char_to_index_map = {\n","        \"'\": 0, \" \": 1, \"a\": 2, \"b\": 3, \"c\": 4, \"d\": 5, \"e\": 6, \"f\": 7, \"g\": 8, \"h\": 9, \"i\": 10,\n","        \"j\": 11, \"k\": 12, \"l\": 13, \"m\": 14, \"n\": 15, \"o\": 16, \"p\": 17, \"q\": 18, \"r\": 19, \"s\": 20,\n","        \"t\": 21, \"u\": 22, \"v\": 23, \"w\": 24, \"x\": 25, \"y\": 26, \"z\": 27, \"_\": 28 # blank character\n","    }\n","\n","    def text_to_int_sequence(text):\n","        int_sequence = []\n","        for char in text.lower():\n","            if char in TextProcessing.char_to_index_map.keys():\n","                index = TextProcessing.char_to_index_map[char]\n","            else: # Ignoring characters not specified in dictionary\n","                continue\n","            int_sequence.append(index)\n","        return int_sequence\n","\n","    def int_sequence_to_text(int_sequence):\n","        text = \"\"\n","        for index in int_sequence:\n","            if index in TextProcessing.index_to_char_map.keys(): # Ignoring integers outside of range specified in dictionary\n","                text += TextProcessing.index_to_char_map[index]\n","        return text\n","\n","    def text_with_only_allowed_characters(text):\n","        output_text = \"\"\n","        for char in text.lower():\n","            if char in TextProcessing.char_to_index_map.keys():\n","                output_text += char\n","        return output_text\n","\n","    def get_char_list():\n","        return list(TextProcessing.char_to_index_map.keys())\n","\n","    def get_index_list():\n","        return list(TextProcessing.index_to_char_map.keys())\n","\n","\n","def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n","\targ_maxes = torch.argmax(output, dim=2)\n","\tdecodes = []\n","\ttargets = []\n","\tfor i, args in enumerate(arg_maxes):\n","\t\tdecode = []\n","\t\ttargets.append(TextProcessing.int_sequence_to_text(labels[i][:label_lengths[i]].tolist()))\n","\t\tfor j, index in enumerate(args):\n","\t\t\tif index != blank_label:\n","\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n","\t\t\t\t\tcontinue\n","\t\t\t\tdecode.append(index.item())\n","\t\tdecodes.append(TextProcessing.int_sequence_to_text(decode))\n","\treturn decodes, targets\n"]},{"cell_type":"markdown","source":["# Audio processing"],"metadata":{"id":"SvLVvtf11POe"}},{"cell_type":"code","source":["class LogMelSpec(nn.Module):\n","\n","    def __init__(self, sample_rate, n_fft, win_length, hop_length, n_mels):\n","        super(LogMelSpec, self).__init__()\n","        self.mel_spectogram_function = torchaudio.transforms.MelSpectrogram(\n","                            sample_rate=sample_rate, n_fft=n_fft,\n","                            win_length=win_length, hop_length=hop_length,\n","                            n_mels=n_mels\n","                            )\n","\n","    def forward(self, x):\n","        x = self.mel_spectogram_function(x)  # mel spectrogram\n","        x = np.log(x + 1e-14)  # logrithmic, add small value to avoid inf\n","        return x\n","\n","class SpecAugment(nn.Module):\n","\n","    def __init__(self, rate, policy=3, freq_mask=15, time_mask=35):\n","        super(SpecAugment, self).__init__()\n","\n","        self.rate = rate\n","\n","        self.specaug = nn.Sequential(\n","            torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask),\n","            torchaudio.transforms.TimeMasking(time_mask_param=time_mask)\n","        )\n","\n","        self.specaug2 = nn.Sequential(\n","            torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask),\n","            torchaudio.transforms.TimeMasking(time_mask_param=time_mask),\n","            torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask),\n","            torchaudio.transforms.TimeMasking(time_mask_param=time_mask)\n","        )\n","\n","        policies = { 1: self.policy1, 2: self.policy2, 3: self.policy3 }\n","        self._forward = policies[policy]\n","\n","    def forward(self, x):\n","        return self._forward(x)\n","\n","    def policy1(self, x):\n","        probability = torch.rand(1, 1).item()\n","        if self.rate > probability:\n","            return  self.specaug(x)\n","        return x\n","\n","    def policy2(self, x):\n","        probability = torch.rand(1, 1).item()\n","        if self.rate > probability:\n","            return  self.specaug2(x)\n","        return x\n","\n","    def policy3(self, x):\n","        probability = torch.rand(1, 1).item()\n","        if probability > 0.5:\n","            return self.policy1(x)\n","        return self.policy2(x)\n","\n","class Data(torch.utils.data.Dataset):\n","    def __init__(self, csv_path, print_errors=False):\n","        print(f\"Loading data CSV file from: {csv_path}\\n\")\n","        self.data = pd.read_csv(csv_path, sep='\\t')\n","\n","        self.print_errors = print_errors\n","\n","        self.audio_transforms = torch.nn.Sequential(\n","            # torchaudio.transforms.MelSpectrogram(sample_rate=SAMPLE_RATE, n_mels=N_MELS)\n","            LogMelSpec(sample_rate=SAMPLE_RATE,\n","                       n_fft=N_FFT,\n","                       n_mels=N_MELS,\n","                       win_length=WIN_LENGTH,\n","                       hop_length=HOP_LENGTH),\n","            SpecAugment(rate=SPECAUG_RATE,\n","                        policy=SPECAUG_POLICY,\n","                        freq_mask=FREQUENCY_MASK,\n","                        time_mask=TIME_MASK)\n","        )\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","\n","    def __getitem__(self, index):\n","        file_path = None\n","        try:\n","            text = self.data.iloc[index].text # Column is named \"text\"\n","            label = TextProcessing.text_to_int_sequence(text) # Text as sequence of ints\n","            label_len = len(label)\n","\n","            file_path = self.data.iloc[index].file # Column is named \"file\"\n","            waveform, samplerate = torchaudio.load(file_path)\n","\n","            spectrogram = self.audio_transforms(waveform) # (channel, feature, time)\n","            spec_len = spectrogram.shape[-1] // 2\n","\n","            if spec_len < label_len:\n","                raise Exception(f'Spectrogram length ({spec_len}) is smaller than label length ({label_len})') # spectrogram length must be higher than label length so that audio is longer than written form\n","\n","        except Exception as e:\n","            if self.print_errors:\n","                print(str(e), file_path, text)\n","                return self.__getitem__(index - 1 if index != 0 else index + 1) # Returning previous item (we have to assume that at least first element was correct, if not we will have a loop)\n","\n","        return spectrogram, label, spec_len, label_len\n","\n","\n","def custom_collate_fn(batch):\n","    spectrograms = []\n","    labels = []\n","    spectrogram_lengths = []\n","    label_lengths = []\n","    for (spectrogram, label, spectrogram_length, label_length) in batch:\n","        if spectrogram is None:\n","            continue\n","\n","        spectrograms.append(spectrogram.squeeze(0).transpose(0, 1))\n","        # Squeeze gets rid of first size parameter beacuse spectrograms in this program are single channel\n","        # FROM: torch.Size([1, 128, 514])    ->    TO: torch.Size([128, 514])\n","        #\n","        # Transposing so that the first parameter will be the number of elements (in our example 514),\n","        # because 128 is number of mels and we are doing this so we can use pad_sequence\n","\n","        labels.append(torch.Tensor(label))\n","        spectrogram_lengths.append(spectrogram_length)\n","        label_lengths.append(label_length)\n","\n","    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n","    # pad_sequence adds zeros  for elements which are smaller than the biggest one, then we unsqueze it\n","    # to get back to this 1 in front of 128, 514 and transpose it to get back from\n","    # [514, 128] to original [128, 514], and we end up with shape: torch.Size([2, 1, 128, 514])\n","    # Where:\n","    #   2 is number of elements in a batch\n","    #   1 is a number of channels of audio\n","    #   128 is number of mels\n","    #   514 is a number of time sequences of this audio (max)\n","\n","    # because batch_first is true number of batches is a first parameter: [2, 1, 128, 514]\n","    # otherwise it will be like this: [514, 1, 128, 2]\n","\n","    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n","\n","    return spectrograms, labels, spectrogram_lengths, label_lengths"],"metadata":{"id":"gJSMVu1X1Ii-","executionInfo":{"status":"ok","timestamp":1707081128598,"user_tz":-60,"elapsed":267,"user":{"displayName":"Tomasz Dołmat","userId":"04722378432579170082"}}},"execution_count":79,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"35nNOG8rrQfs"},"source":["# Model"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"st-G2OObrW5F","executionInfo":{"status":"ok","timestamp":1707080848633,"user_tz":-60,"elapsed":287,"user":{"displayName":"Tomasz Dołmat","userId":"04722378432579170082"}}},"outputs":[],"source":["class TransposeLayer(nn.Module):\n","    def __init__(self, dim0, dim1):\n","        super(TransposeLayer, self).__init__()\n","        self.dim0 = dim0\n","        self.dim1 = dim1\n","\n","    def forward(self, x):\n","        x = x.transpose(self.dim0, self.dim1)\n","        return x\n","\n","\n","class SpeechRecognitionModel(nn.Module):\n","\n","    def __init__(self, device=\"cpu\"):\n","        super(SpeechRecognitionModel, self).__init__()\n","\n","        use_cuda = torch.cuda.is_available()\n","        self.device = device\n","\n","        self.criterion = nn.CTCLoss(blank=28, zero_infinity=True)\n","        self.learning_rate = 1e-3\n","\n","        self.validation_step_outputs = []\n","\n","        self.cnn = nn.Sequential(\n","            nn.Conv1d(N_MELS, N_MELS, kernel_size=KERNEL_SIZE, stride=STRIDE, padding=KERNEL_SIZE//STRIDE),\n","            TransposeLayer(1, 2),\n","            nn.LayerNorm(N_MELS),\n","            nn.GELU(),\n","            nn.Dropout(DROPOUT),\n","        )\n","        self.dense = nn.Sequential(\n","            nn.Linear(N_MELS, 128),\n","            nn.LayerNorm(128),\n","            nn.GELU(),\n","            nn.Dropout(DROPOUT),\n","            nn.Linear(128, 128),\n","            nn.LayerNorm(128),\n","            nn.GELU(),\n","            nn.Dropout(DROPOUT),\n","        )\n","        self.lstm = nn.LSTM(input_size=128, hidden_size=LSTM_HIDDEN_SIZE,\n","                            num_layers=LSTM_NUMBER_OF_LAYERS, dropout=LSTM_DROPOUT,\n","                            bidirectional=LSTM_BIDIRECTIONAL, batch_first=True)\n","        self.final_transformations = nn.Sequential(\n","            nn.LayerNorm(LSTM_HIDDEN_SIZE),\n","            nn.GELU(),\n","            nn.Dropout(DROPOUT),\n","        )\n","\n","        self.final_fc = nn.Linear(LSTM_HIDDEN_SIZE, NUMBER_OF_CLASSES) # final fully connected\n","\n","    def forward(self, x):\n","        current_batch_size = x.shape[0]\n","\n","        h_0 = torch.zeros(1, current_batch_size, LSTM_HIDDEN_SIZE).to(self.device)\n","        c_0 = torch.zeros(1, current_batch_size, LSTM_HIDDEN_SIZE).to(self.device)\n","\n","        x = x.squeeze(1)  # batch, feature, time - removing unnecessary dimention for num_of_channels\n","        x = self.cnn(x) # batch, time, feature\n","        x = self.dense(x) # batch, time, feature\n","        x, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n","\n","        x = self.final_transformations(x)  # (batch, time, n_class)\n","        x = self.final_fc(x)\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"CuguNEzKnMOn"},"source":["# Training and Validation"]},{"cell_type":"code","execution_count":83,"metadata":{"id":"ydkqGeOwnPGY","executionInfo":{"status":"ok","timestamp":1707081198012,"user_tz":-60,"elapsed":267,"user":{"displayName":"Tomasz Dołmat","userId":"04722378432579170082"}}},"outputs":[],"source":["class IterationsCounter():\n","    def __init__(self):\n","        self.value = 0\n","\n","    def step(self):\n","        self.value += 1\n","\n","    def get(self):\n","        return self.value\n","\n","\n","def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iterations_counter, experiment):\n","    model.train()\n","    data_len = len(train_loader.dataset)\n","\n","    for batch_idx, _data in enumerate(train_loader):\n","        spectrograms, labels, input_lengths, label_lengths = _data\n","        spectrograms, labels = spectrograms.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        output = model(spectrograms)  # (batch, time, n_class)\n","        output = F.log_softmax(output, dim=2)\n","        output = output.transpose(0, 1) # (time, batch, n_class)\n","\n","        loss = criterion(output, labels, input_lengths, label_lengths)\n","        loss.backward()\n","\n","        optimizer.step()\n","        scheduler.step()\n","        iterations_counter.step()\n","        if batch_idx % 100 == 0 or batch_idx == data_len:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(spectrograms), data_len,\n","                100. * batch_idx / len(train_loader), loss.item()))\n","            if batch_idx % 1000 == 0 or batch_idx == data_len:\n","                decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n","                print(decoded_preds)\n","                print(decoded_targets)\n","\n","\n","def test(model, device, test_loader, criterion, epoch, iterations_counter, experiment):\n","    print('\\nevaluating...')\n","    model.eval()\n","    test_loss = 0\n","    test_cer, test_wer = [], []\n","    with torch.no_grad():\n","        for i, _data in enumerate(test_loader):\n","            spectrograms, labels, input_lengths, label_lengths = _data\n","            spectrograms, labels = spectrograms.to(device), labels.to(device)\n","\n","            output = model(spectrograms)  # (batch, time, n_class)\n","            output = F.log_softmax(output, dim=2)\n","            output = output.transpose(0, 1) # (time, batch, n_class)\n","\n","            loss = criterion(output, labels, input_lengths, label_lengths)\n","            test_loss += loss.item() / len(test_loader)\n","\n","            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n","            for j in range(len(decoded_preds)):\n","                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n","                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n","\n","    avg_cer = sum(test_cer)/len(test_cer)\n","    avg_wer = sum(test_wer)/len(test_wer)\n","\n","    print(f'Test set: Average loss: {test_loss:.4f}, Average CER: {avg_cer:4f} Average WER: {avg_wer:.4f}\\n')\n","\n","\n","def main(learning_rate=1e-3, batch_size=20, epochs=10):\n","\n","    use_cuda = torch.cuda.is_available()\n","    torch.manual_seed(7)\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","    print(f\"Device in use: {device}\\n\")\n","\n","    if not os.path.isdir(\"./data\"):\n","        os.makedirs(\"./data\")\n","\n","    train_dataset = Data(csv_path=TRAIN_PATH)\n","    test_dataset = Data(csv_path=VALIDATE_PATH)\n","\n","    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n","    train_loader = data.DataLoader(dataset=train_dataset,\n","                                batch_size=BATCH_SIZE,\n","                                shuffle=True,\n","                                collate_fn=custom_collate_fn,\n","                                **kwargs)\n","    test_loader = data.DataLoader(dataset=test_dataset,\n","                                batch_size=BATCH_SIZE,\n","                                shuffle=False,\n","                                collate_fn=custom_collate_fn,\n","                                **kwargs)\n","\n","    model = SpeechRecognitionModel(device=device).to(device)\n","\n","    LOADED_EPOCH = 40\n","    LOAD_MODEL_FILEPATH = f\"{SAVE_MODEL_PATH}/model-{LOADED_EPOCH}\"\n","\n","\n","    if LOAD_MODEL_FILEPATH and len(LOAD_MODEL_FILEPATH) > 1:\n","        print(f\"Model loaded from {LOAD_MODEL_FILEPATH}\\n\")\n","        model.load_state_dict(torch.load(LOAD_MODEL_FILEPATH))\n","\n","    # print(model)\n","\n","    optimizer = optim.AdamW(model.parameters(), learning_rate)\n","    criterion = nn.CTCLoss(blank=BLANK_CHARACTER_INDEX).to(device)\n","    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate,\n","                                            steps_per_epoch=int(len(train_loader)),\n","                                            epochs=epochs,\n","                                            anneal_strategy='linear')\n","\n","    if not os.path.isdir(SAVE_MODEL_PATH):\n","        os.makedirs(SAVE_MODEL_PATH)\n","\n","    iterations_counter = IterationsCounter()\n","    for epoch in range(1, epochs + 1):\n","        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iterations_counter, None)\n","        test(model, device, test_loader, criterion, epoch, iterations_counter, None)\n","\n","        torch.save(model.state_dict(), f\"{SAVE_MODEL_PATH}/model-{epoch + LOADED_EPOCH}\")\n"]},{"cell_type":"markdown","metadata":{"id":"hXvlWZeVpXfX"},"source":["# Run Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZodve8PGKfS"},"outputs":[],"source":["learning_rate = 1e-3\n","\n","main(learning_rate, BATCH_SIZE, MAX_EPOCHS)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}